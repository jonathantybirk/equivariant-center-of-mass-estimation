# Example configuration for GNN training with Weights & Biases logging
# Usage: python train_gnn_optimized.py fit --config config_wandb.yaml

# Model configuration
model:
  # class_path: BasicGNN
  # init_args:
  #   lr: 1e-3
  #   weight_decay: 1e-5
    # dropout: 0.1
  class_path: EquivariantGNN
  # class_path: EquivariantGNNFast
  init_args:
    lr: 1e-3
    weight_decay: 1e-5
    multiplicity: 2
    message_passing_steps: 3
    max_sh_degree: 2
    base_l_values: [0, 1, 2]

# Data configuration (since we specified datamodule_class=PointCloudData, we don't need class_path)
data:
  # data_dir: "data/processed_dv"
  data_dir: "data/processed_sh2"
  val_split: 0.2
  batch_size: 16
  num_workers: 0

# Trainer configuration with W&B logging
trainer:
  max_epochs: 100
  accelerator: auto
  devices: auto
  precision: 32
  # Weights & Biases logger configuration
  # logger:
  #   class_path: lightning.pytorch.loggers.WandbLogger
  #   init_args:
  #     project: "gnn-center-of-mass"
  #     name: "equivariant-gnn-optimized"
  #     save_dir: "wandb_logs"
  #     log_model: "all"  # Log model checkpoints to W&B
  #     tags: ["equivariant", "gnn", "center-of-mass", "optimized"]
  
  # Callbacks for better training
  callbacks:
    # Custom callback to log total parameters to WandB metadata
    # - class_path: src.callbacks.parameter_logger.ParameterCountLogger
    
    # Model summary with detailed submodule parameter breakdown
    - class_path: lightning.pytorch.callbacks.ModelSummary
      init_args:
        max_depth: -1  # Show ALL submodules (CGWeight, MessageLayer, etc.)
        
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_displacement_distance"
        mode: "min"
        save_top_k: 1
        filename: "best-{epoch}-{val_displacement_distance:.4f}"
        
    # - class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     monitor: "val_mae"
    #     mode: "min"
    #     patience: 20
    #     min_delta: 1e-6
        
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch" 