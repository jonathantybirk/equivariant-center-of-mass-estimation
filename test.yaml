# Example configuration for GNN training with Weights & Biases logging
# Usage: python train_gnn_optimized.py fit --config config_wandb.yaml

# Model configuration
model:
  # class_path: BasicGNN
  class_path: EquivariantGNN
  init_args:
    lr: 1e-3
    weight_decay: 1e-5
    input_dim: 3
    hidden_dim: 16
    message_passing_steps: 3
    final_mlp_dims: [64, 32]
    max_sh_degree: 1

# Data configuration (since we specified datamodule_class=PointCloudData, we don't need class_path)
data:
  # data_dir: "data/processed_dv"
  data_dir: "data/processed_sh"
  val_split: 0.1
  batch_size: 16
  num_workers: 0

# Trainer configuration with W&B logging
trainer:
  max_epochs: 10
  accelerator: auto
  devices: auto
  precision: 32
  
  # Weights & Biases logger configuration
  # logger:
  #   class_path: lightning.pytorch.loggers.WandbLogger
  #   init_args:
  #     project: "gnn-center-of-mass"
  #     name: "equivariant-gnn-optimized"
  #     save_dir: "wandb_logs"
  #     log_model: "all"  # Log model checkpoints to W&B
  #     tags: ["equivariant", "gnn", "center-of-mass", "optimized"]
  
  # Callbacks for better training
  callbacks:
    # Model summary with detailed submodule parameter breakdown
    - class_path: lightning.pytorch.callbacks.ModelSummary
      init_args:
        max_depth: -1  # Show ALL submodules (CGWeight, MessageLayer, etc.)
        
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_mae"
        mode: "min"
        save_top_k: 1
        filename: "best-{epoch}-{val_mae:.4f}"
        
    # - class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     monitor: "val_mae"
    #     mode: "min"
    #     patience: 20
    #     min_delta: 1e-6
        
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch" 