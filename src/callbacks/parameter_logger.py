"""
Custom callback to log total learnable parameters to WandB metadata
"""

import pytorch_lightning as pl
from pytorch_lightning.utilities import rank_zero_only


class ParameterCountLogger(pl.Callback):
    """
    Callback to log total number of learnable parameters to WandB metadata
    """

    def __init__(self):
        super().__init__()

    @rank_zero_only
    def on_train_start(self, trainer, pl_module):
        """Log parameter count at the start of training"""
        # Count total parameters
        total_params = sum(p.numel() for p in pl_module.parameters())
        trainable_params = sum(
            p.numel() for p in pl_module.parameters() if p.requires_grad
        )

        # Get parameter breakdown by module
        param_breakdown = {}
        for name, module in pl_module.named_modules():
            if len(list(module.parameters())) > 0:
                module_params = sum(p.numel() for p in module.parameters())
                param_breakdown[f"params/{name}"] = module_params

        # Log to WandB via the logger
        if hasattr(trainer.logger, "experiment"):
            # Log to WandB config (metadata)
            trainer.logger.experiment.config.update(
                {
                    "model/total_parameters": total_params,
                    "model/trainable_parameters": trainable_params,
                    "model/non_trainable_parameters": total_params - trainable_params,
                    **param_breakdown,
                }
            )

            # Also log as summary metrics for easy access
            trainer.logger.experiment.summary.update(
                {
                    "total_parameters": total_params,
                    "trainable_parameters": trainable_params,
                }
            )

        print(f"ðŸ“Š Model Parameters:")
        print(f"  Total: {total_params:,}")
        print(f"  Trainable: {trainable_params:,}")
        print(f"  Non-trainable: {total_params - trainable_params:,}")
