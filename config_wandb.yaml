# Example configuration for GNN training with Weights & Biases logging
# Usage: python train_gnn_optimized.py fit --config config_wandb.yaml

# Model configuration
model:
  class_path: EquivariantGNN
  init_args:
    hidden_dim: 128
    num_layers: 4
    lr: 1e-3
    weight_decay: 1e-5

# Data configuration  
data:
  class_path: PointCloudData
  init_args:
    batch_size: 32
    data_dir: "data/processed_sh"
    val_split: 0.1
    num_workers: 0

# Trainer configuration with W&B logging
trainer:
  max_epochs: 100
  accelerator: auto
  devices: auto
  precision: 32
  
  # Weights & Biases logger configuration
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "gnn-center-of-mass"
      name: "equivariant-gnn-optimized"
      save_dir: "wandb_logs"
      log_model: "all"  # Log model checkpoints to W&B
      tags: ["equivariant", "gnn", "center-of-mass", "optimized"]
  
  # Callbacks for better training
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: "val_mae"
        mode: "min"
        save_top_k: 3
        filename: "best-{epoch}-{val_mae:.4f}"
        
    # - class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     monitor: "val_mae"
    #     mode: "min"
    #     patience: 20
    #     min_delta: 1e-6
        
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: "epoch" 