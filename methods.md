We will construct our dataset from realistic 3D meshes of everyday household objects obtained through free online asset collections, such as on the Unity Asset Store. For each object, we will calculate the exact center of mass, assuming uniform density and solid materials. To simulate realistic LiDAR point clouds, we will place between one and five virtual LiDAR sensors at random positions around each object and cast rays toward the object surface, recording intersection points.
We will implement an E(n) Equivariant Graph Neural Network (EGNN) architecture to predict the center of mass directly from these point clouds. The EGNN will incorporate steerable equivariant layers, thereby ensuring strict equivariance under rotations and translations.
For baseline comparisons, we will train a standard (non-equivariant) Graph Neural Network (GNN). To explicitly isolate the benefit provided by architectural equivariance, we will additionally train a non-equivariant GNN with extensive rotation and translation data augmentation. Furthermore, we will evaluate two geometric baselines: a simple centroid calculated as the average of all point positions, and a stronger geometric baseline calculated as the centroid of the convex hull fitted around each point cloud.
All neural network models will be trained by minimizing the mean squared Euclidean distance between predicted and exact centers of mass, using gradient descent optimization. Each neural network's training will be limited to a maximum of 10 hours, with hyperparameters selected via random search based on validation set performance.
To quantitatively evaluate model equivariance, we will additionally apply random rotations and translations to test point clouds. For each transformed point cloud, we will measure how closely the model's prediction aligns with the expected transformed center of mass under equivariance. This explicitly assesses whether predicted centers of mass consistently shift with input transformations.
